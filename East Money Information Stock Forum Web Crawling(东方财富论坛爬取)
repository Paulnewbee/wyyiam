from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from tdqm import tdqm

def web_crawling_v2(browser,stock_num):
    
    global combo
    
    temp_web = 'http://guba.eastmoney.com/list,601985,f_2.html'          
    
    #the f_ means it is based on the post time if you want to change it and let it be based on comment time, just delete ‘f_’  and change the code at 16,20 line
    
    temp_num = str(stock_num)
    temp_last = temp_web.split(',')[2] 
    old_str = '_2'
    new_web = temp_web.split(',')[0] + ',' + temp_num + ',' + temp_web.split(',')[2]
    browser.get(new_web)
    a = len(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div'))
    for x in tdqm(range(3,int((browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')[a-2].
                           find_element_by_xpath('span/span/span').text)))):
        temp_str = '_' + str(x)
        temp_web = temp_web.replace(old_str,temp_str)
        old_str = temp_str
        new_web = temp_web.split(',')[0] + ',' + temp_num + ',' + temp_web.split(',')[2]
        browser.get(new_web)
        for i in range(1,a-2):
            temp = []
            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                 [i].find_elements_by_xpath('span')[0].text)

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                [i].find_elements_by_xpath('span')[1].text)

            try:
                temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                            [i].find_elements_by_xpath('span')[2].find_element_by_xpath('em').text)
            except:
                temp.append(None)

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                         [i].find_elements_by_xpath('span')[2].find_element_by_xpath('a').text)

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                [i].find_elements_by_xpath('span')[2].find_element_by_xpath('a').get_attribute('href'))

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                [i].find_elements_by_xpath('span')[3].text)

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                 [i].find_elements_by_xpath('span')[3].find_elements_by_xpath('a')[0].get_attribute('data-poptype'))

            temp.append((browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                                [i].find_elements_by_xpath('span')[3].find_elements_by_xpath('a')[0].get_attribute('href')))

            temp.append(browser.find_elements_by_xpath('//*[@id="articlelistnew"]/div')
                        [2].find_elements_by_xpath('span')[4].text)


            for z in range(0,len(combo)):
                combo[z].append(temp[z])
                        #print(i)
                    
browser = webdriver.Chrome('your_path')
reading_times = []
comments_num = []
tips = []
title = []
article_link = []
author_name = []
author_status = []
author_link = []
date = []
combo = [reading_times,comments_num,tips,title,article_link,author_name,author_status,author_link,date]
name = ['reading_times','comments_num','tips','title','article_link','author_name','author_status','author_link','date']
file_name = 'whatever_name_you_want.csv'

web_crawling_v2(browser,stock_num)   #stock_num needs a 'int' type data



def save_csv(combo,name,file_name):
    com = pd.DataFrame(combo).T
    com.columns = name
    com.to_csv(file_name)
    
save_csv(combo,name,file_name)
